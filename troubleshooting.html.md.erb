---
title: Troubleshooting and Diagnostics
owner: MySQL
---

This topic describes how to diagnose and troubleshoot problems with MySQL for Pivotal Cloud Foundry (PCF).

## <a id="diagnosing"></a> Diagnose Problems

If your cluster is experiencing downtime or is in a degraded state, Pivotal recommends the following workflow
for gathering information to diagnose the type of failure the cluster is experiencing.

1. Use the [mysql-diag](mysql-diag.html) tool to gather a summary of the network, disk, and replication state of each cluster node.
1. Run [download-logs](https://discuss.pivotal.io/hc/en-us/articles/221504408-Script-to-download-logs-from-MYSQL-server-tile-for-further-troubleshooting) against each node in your MySQL cluster,
   the MySQL proxies, and the MySQL backup-prepare node.
   It is important to do this before attempting recovery,
   because any failures in the recovery procedure can result in logs being lost or made inaccessible.<br><br> 

    It is recommended that you use the `-X` flag to get the complete set of available logs.
   However, if your cluster processes a high volume of transactions, the complete set may be too large and you can omit this flag to fetch the essential set of logs.
1. Depending on the reported state from `mysql-diag`, the troubleshooting techniques listed in the [Common Problems](#common-problems) section below might allow you to recover your cluster.
1. If you are uncertain about the recovery steps to take, submit a ticket through [Pivotal Support](https://support.pivotal.io/)
   with the following information:
  - Logs fetched by running the `download-logs` script
  - Output from `mysql-diag`
  - Type of environment the MySQL deployment is running in (Elastic Runtime, MySQL for PCF, or other)
  - Versions of the installed Ops Manager, Elastic Runtime, and MySQL for PCF

For more diagnostic techniques, see [Diagnostic Techniques](#diagnostic-techniques).
  
## <a id="diagnostic-techniques"></a> Diagnostic Techniques

After performing the procedure in [Diagnose Problems](#diagnosing), consult the following additional diagnostic techniques to learn more about your cluster.

### <a id="check-state"></a> Check Cluster State

`mysql_diag` reports the cluster's state. However, you can check the state manually by connecting to each MySQL node using a MySQL client and checking its status:

<pre class="terminal">
$ mysql -h NODE_IP -u root -pPASSWORD -e 'SHOW STATUS LIKE "wsrep_cluster_status";'
+----------------------+---------+
| Variable_name        | Value   |
+----------------------+---------+
| wsrep_cluster_status | Primary |
+----------------------+---------+
</pre>

If all nodes are in the `Primary` component, you have a healthy cluster.
If some nodes are in a `Non-primary` component, those nodes are not able to join the cluster.

To check how many nodes are in the cluster, perform the following command:

<pre class="terminal">
$ mysql -h NODE_IP -u root -pPASSWORD -e 'SHOW STATUS LIKE "wsrep_cluster_size";'
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| wsrep_cluster_size | 3     |
+--------------------+-------+
</pre>

If the value of `wsrep_cluster_size` is equal to the expected number of nodes, then all nodes have joined the cluster.
Otherwise, check network connectivity between nodes and use `monit status` to identify any issues preventing nodes from starting.

For more information, see the official Galera documentation for [Checking Cluster Integrity](http://galeracluster.com/documentation-webpages/monitoringthecluster.html#checking-cluster-integrity).

If none of your nodes are in the `Primary` component, then your cluster has lost quorum and must be bootstrapped.

For more information, see [Bootstrapping](bootstrapping.html) for more information.

### <a id="check-replication"></a>Check Replication Status

If you see stale data in your cluster, check whether replication is 
functioning normally. 

Perform the following steps to check the replication status:

1. Obtain the IP addresses of your MySQL server by performing the following steps:
  1. From the PCF **Installation Dashboard**, click the **MySQL for Pivotal Cloud Foundry** tile.
  1. Click the **Status** tab.
  1. Record the IP addresses for all instances of the **MySQL Server** job.

1. Obtain the admin credentials for your MySQL server by performing the following steps:
  1. From the **MySQL for PCF** tile, click the **Credentials** tab.
  1. Locate the **Mysql Admin Password** entry in the **MySQL Server** section and click **Link to Credential**.
  1. Record the values for `identity` and `password`.

1. SSH into the Ops Manager VM.
   Because the procedures vary by IaaS, see <a href="http://docs.pivotal.io/pivotalcf/1-11/customizing/trouble-advanced.html#ssh">SSH into Ops Manager</a> for more information.

1. From the Ops Manager VM, place some data in the first node by performing the following steps, replacing `FIRST-NODE-IP-ADDRESS` with the IP address of the first node retrieved from the initial step and `YOUR-IDENTITY` with the `identity` value obtained from the second step.
   When prompted for a password, provide the `password` value obtained from the second step.
  1. Create a dummy database in the first node:
    <pre class="terminal">
    $ mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -e "create database verify\_healthy;"
    </pre>
  1. Create a dummy table in the dummy database:
    <pre class="terminal">
    $ mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "create table dummy_table (id int not null primary key auto\_increment, info text) engine='InnoDB';"
    </pre>
  1. Insert some data into the dummy table:
    <pre class="terminal">
    $ mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "insert into dummy_table(info) values ('dummy data'),('more dummy data'),('even more dummy data');"
    </pre>
  1. Query the table and verify that the three rows of dummy data exist on the first node:
    <pre class="terminal">
    mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "select * from dummy\_table;"
    Enter password:
    +----+----------------------+
    | id | info                 |
    +----+----------------------+
    |  4 | dummy data           |
    |  7 | more dummy data      |
    | 10 | even more dummy data |
    +----+----------------------+
  </pre>

1. Verify that the other nodes contain the same dummy data by performing the following steps for each of the remaining MySQL server IP addresses obtained above:
  1. Query the dummy table, replacing `NEXT-NODE-IP-ADDRESS` with the IP address of the MySQL server instance and `YOUR-IDENTITY` with the `identity` value obtained above. When prompted for a password, provide the `password` value obtained above.
    <pre class="terminal">
    $ mysql -h NEXT-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "select * from dummy\_table;"
    </pre>
  1. Examine the output of the `mysql` command and verify that the node contains the same three rows of dummy data as the other nodes.
    <pre class="terminal">
    +----+----------------------+
    | id | info                 |
    +----+----------------------+
    |  4 | dummy data           |
    |  7 | more dummy data      |
    | 10 | even more dummy data |
    +----+----------------------+
    </pre>

1. If each MySQL server instance does not return the same result, contact [Pivotal Support](https://support.pivotal.io/) before proceeding further or making any changes to your deployment.
   If each MySQL server instance returns the same result, then you can safely proceed to scaling down your cluster to a single node.

## <a id='common-problems'></a>Troubleshoot Common Problems

This section lists symptoms, solutions, and explanations for the following common problems:

* Cluster errors:
  + [Many Replication Errors in Logs](#replication-errors)
  + [No Space Left](#no-space)
  + [Node Unable to Rejoin](#wont-rejoin)
  + [Unresponsive Node(s)](#unresponsive)
* User errors:
  + [Accidental Deletion of Service Plan](#service-plan-deletion)

### <a id="replication-errors"></a> Many Replication Errors in Logs

#### Symptom

You see many replication errors in the MySQL logs.

#### Explanation

Replication errors in your MySQL logs can also correspond to execution errors in replication logs (`GRA_*.log` files) during problems such as running out of disk space.

Seeing many replication errors in your MySQL logs can also be part of normal behavior.
For example, MySQL logs can show replication errors when an app issues an `ALTER TABLE` command that fails to apply to the current schema.
This is usually the result of user error.
Resulting MySQL log lines may resemble the following:

<pre class="terminal">
160318 9:25:16 [Warning] WSREP: RBR event 1 Query apply warning: 1, 16992456
160318 9:25:16 [Warning] WSREP: Ignoring error for TO isolated action: source: abcd1234-abcd-1234-abcd-1234abcd1234 version: 3 local: 0 state: APPLYING flags: 65 conn_id: 246804 trx_id: -1 seqnos (l: 865022, g: 16992456, s: 16992455, d: 16992455, ts: 2530660989030983)
160318 9:25:16 [ERROR] Slave SQL: Error 'Duplicate column name 'number'' on query. Default database: 'cf_0123456_1234_abcd_1234_abcd1234abcd'. Query: 'ALTER TABLE ...'
</pre>

This error occurs when an app issues an `ALTER TABLE` command that fails to apply to the current schema.
This is typically the result of user error.

The MySQL node that receives the request processes it normally.
If it fails, the node sends the failure to the app and the app must determine the next step. 
In a Galera cluster, however, all DDL is replicated and all replication failures are logged.
Therefore, the bad `ALTER TABLE` command is run by all nodes.
If it fails, the slave nodes log it as a "replication failure."

Cases where a valid DDL works on some nodes yet fails on others are rare.
Usually those cases are limited to problems with running out of disk space or working memory. 

An article from [Percona](https://www.percona.com/blog/2014/07/21/a-schema-change-inconsistency-with-galera-cluster-for-mysql/) suggests that the schemata can get out of sync.
However, the author had to deliberately switch a node to RSU, which MySQL for PCF never does except during SST.
The article offers a demonstration of what is possible, but does not explain how a customer may actually experience this issue in production.

#### Solution

Disregard the replication errors as a normal behavior.
If you see the "ALTER TABLE" error, fix the user error that produced it.

### <a id="no-space"></a> No Space Left

#### Symptom

You may encounter one or more of the following symptoms when MySQL has run out of space: 

* MySQL gives an error that reports it is out of space
* Queries start failing
* Apps are unable to connect
* You are unable to SSH into a node

#### Explanation

When MySQL runs out of space, it stops functioning normally.

#### Solution

1. Use the [mysql-diag](mysql-diag.html) tool to determine how much space you have left. If the disk is too full, the `mysql-diag` won't work.
1. Redeploy with more persistent disk.
1. After redeploying, you may encounter further errors. To troubleshoot these errors, see the other sections in [Troubleshoot Common Problems](#common-problems).

### <a id="wont-rejoin"></a> Node Unable to Rejoin

#### Symptom

A MySQL node is unable to rejoin a cluster.

#### Explanation

Existing server nodes restarted with `monit` should automatically join the cluster.
If a detached existing node fails to join the cluster, it might be because its sequence number (`seqno`) is higher than those of the nodes with quorum.

A higher `seqno` on the detached node indicates that it has recent changes to the data that the primary lacks.
Check this by looking at the node's error log at `/var/vcap/sys/log/mysql/mysql.err.log`.

#### Solution

If the detached node has a higher sequence number than the primary component, perform one of the following procedures to restore the cluster:

  - Shut down the healthy, primary nodes and bootstrap from the detached node to preserve its data.
    See [Bootstrapping](bootstrapping.html) for more details.
  - Abandon the unsynchronized data on the detached node by performing the manual procedure to force the node to rejoin the cluster, documented in [Pivotal Knowledge Base](https://discuss.pivotal.io/hc/en-us/articles/115014258668). 

    <p class="note warning"><strong>WARNING</strong>: Forcing a node to rejoin the cluster is a destructive procedure. Only perform it with the assistance of <a href="https://support.pivotal.io">Pivotal Support</a>.</p>

It might also be possible to manually dump recent transactions from the detached node and apply them to the running cluster, but this process is error-prone.

### <a id="unresponsive"></a> Unresponsive Node(s)

#### Symptom

A MySQL node has become unresponsive.

#### Explanation

If the client is connected to a MySQL cluster node that loses connection to the rest of the cluster (becomes non-Primary), the node stops accepting writes.
If the connection to this node is made through the proxy, the proxy should re-route further connections to a different node.

A node can become unresponsive for a number of reasons. Each reason corresponds to a different solution. Consult the following list:

- [Network Latency](#network-latency)
- [MySQL Process Failure](#mysql-process-failure)
- [Firewall Rule Change](#firewall-rule-change)
- [VM Failure](#vm-failure)
- [The Interruptor](#interruptor) 

#### <a id='network-latency'></a>Network Latency

If network latency causes a node to become unresponsive, the node drops but eventually rejoins.

The node will only rejoin automatically if only one node has left the cluster.

Consult your IaaS network settings to reduce your network latency.

#### <a id='mysql-process-failure'></a>MySQL Process Failure

If the MySQL process crashes, monit and BOSH will bring the process back. No further action is necessary.

#### <a id='firewall-rule-change'></a>Firewall Rule Change

If firewall rules change, it may prevent a node from reaching the rest of the cluster, causing the node to become unresponsive.  

In this scenario, the logs show the node leaving the cluster but do not show network latency errors.

To confirm that the node is unresponsive due to a firewall rule change, try to SSH from a responsive node to the unresponsive node. If you can't connect, the node is unresponsive due to a firewall rule change.

Change the firewall rules to enable the unresponsive node to rejoin the cluster.

#### <a id='vm-failure'></a>VM Failure

If you cannot SSH onto a node, and you are not detecting either network 
latency or firewall issues, your node may be down due to VM failure.

To confirm that VM failure has caused the node to become unresponsive, perform the following steps:

1. SSH into the Ops Manager Director. For more information, see the [SSH into Ops Manager](http://docs.pivotal.io/pivotalcf/1-11/customizing/trouble-advanced.html#ssh) section in <em>Advanced Troubleshooting with the BOSH CLI</em>.
1. Retrieve the IP address for the MySQL server by navigating to the **MySQL for PCF** tile and clicking the **Status** tab.
1. Retrieve the VM credentials for the MySQL server by navigating to the **MySQL for PCF** tile and clicking the **Credentials** tab.
1. From the Ops Manager Director VM, use the BOSH CLI to log in to the BOSH Director:
  * If you are using PCF v1.10, follow the BOSH CLI v1 procedure in [Log in to the BOSH Director](http://docs.pivotal.io/pivotalcf/1-11/customizing/trouble-advanced.html#bosh1-access).
  * If you are using PCF v1.11 or later, follow the BOSH CLI v2 procedure in [Log in to the BOSH Director](http://docs.pivotal.io/pivotalcf/1-11/customizing/trouble-advanced.html#bosh2-access).
1. From the Ops Manager VM, use the BOSH CLI to run `bosh cloudcheck`. Follow the procedure in [BOSH Cloudcheck](https://docs.pivotal.io/pivotalcf/1-11/customizing/trouble-advanced.html#cck). If you are using PCF v1.10, use the BOSH CLI v1 commands. If you are using PCF v1.11 or later, use the BOSH CLI v2 commands.
1. In `bosh cloudcheck`, identify the unresponsive node and recreate it.
  <p class="note warning"><strong>WARNING</strong>: Recreating a node will clear its logs. Ensure the node is completely down before recreating it.</p>
  <p class="note warning"><strong>WARNING</strong>: Only recreate one node. Do not recreate the entire cluster. If more than one node is down, contact [Pivotal Support](https://support.pivotal.io).</p>

#### <a id='interruptor'></a>The Interruptor

If you have enabled the Interruptor, it may be preventing a node from starting. 

You can confirm that the Interruptor has activated by examining `/var/vcap/sys/log/mysql/mysql.err.log` on the failing node. The log contains the following message:

```
WSREP_SST: [ERROR] ##################################################################################### (20160610 04:33:21.338)
WSREP_SST: [ERROR] SST disabled due to danger of data loss. Verify data and run the rejoin-unsafe errand (20160610 04:33:21.340)
WSREP_SST: [ERROR] ##################################################################################### (20160610 04:33:21.341)
```

Perform the procedure in the [Override the Interruptor](interruptor.html#force-rejoin) section of <em>Using the Interruptor</em> to force the node to rejoin the cluster.

### <a id="service-plan-deletion"></a> Accidental Deletion of Service Plan ##

#### Symptom

You have deleted a service plan accidentally.

#### Explanation

A service plan is only unrecoverable if you perform all of the following steps in sequence:

1. Click the trash-can icon in the **Service Plan** section of the MySQL for PCF configuration.
1. Create a plan with the same name.
1. Click **Save**.
1. Return to the Ops Manager Installation Dashboard, and click **Apply Changes**.

The deploy eventually fails with the following error:

<pre class="terminal">
Server error, status code: 502, error code: 270012, message: Service broker catalog is invalid: Plan names must be unique within a service
</pre>

#### Solution

After you have deployed with **Apply Changes**, the original plan cannot be recovered.
For as long as service instances of that plan exist, you may not enter a new plan of the same name.
At this point, the only workaround is to create a new plan with the same specifications, but specify a different name.
Existing instances continue to appear under the old plan name, but new instances need to be created using the new plan name.

If you have performed steps 1 and 2 in the above sequence, do not click **Save**.
Return to the Ops Manager Installation Dashboard.
Any accidental changes are discarded.

If you have performed steps 1, 2, and 3 in the above sequence, do not click **Apply Changes.**
Return to the **Ops Manager Installation Dashboard** and click the **Revert** button.
Any accidental changes are discarded.

