---
title: Configuring MySQL for PCF
owner: MySQL
---

This topic explains how to configure the MySQL for PCF service.

To configure your MySQL service, click the **MySQL for PCF** tile in the Ops Manager **Installation Dashboard**, open each pane under the **Settings** tab, and review or change the configurable settings as described in the sections below.

![Configure Settings](config-settings.png)

## <a id="az-network"></a>Assign AZs and Networks

![Configure AZs and Networks](config-az-network.png)

MySQL for PCF supports deployment to multiple availability zones (AZs).

To maximize uptime, deploy a load balancer in front of the SQL Proxy nodes. Please see the note in the [proxy](#proxy) section below. When configuring this load balancer, increase the minimum idle timeout if possible, as many load balancers are tuned for short-lived connections unsuitable for long-running database queries. See the [load balancer configuration instructions](./installing.html#load-balancer) for details.

## <a id="plans"></a>Service Plans

![Configure Service Plans](config-service-plans.png)

Service plans offer developers different versions of the MySQL service. An example is tiered service plans with that offer a range of resource limits and pricing. See [Service Plans](./service-plan.html) for how to configure one or more service plans in the **Service Plans** pane.

<p class="note"><strong>Note</strong>: You cannot deploy MySQL for PCF without at least one service plan defined.</p>

## <a id="proxy"></a>Proxy

![Configure Proxy](config-proxy.png)

The proxy tier routes connections from apps to healthy MariaDB cluster nodes, even in the event of node failure.

- In the **Proxy IPs** field, enter a list of IP addresses that should be assigned to the proxy instances.  
    * These IP addresses must be in the CIDR range configured in the Director tile and not be currently allocated to another VM. 
    * Look at the **Status** pages of other tiles to see what IP addresses are in use.  
    * You must make the **Proxy** instance count in the [Resource Config tab](#resource-config) the same as the number of IP addresses you enter in this field.
    * For high availability, you must also ensure you increase the number of **MySQL Server** instances from one to three in the [Resource Config tab](#resource-config). For more information on high availability, see [Switch Between Single and HA Topologies](#switch-topologies).

- In the **Binding Credentials Hostname** field, enter the hostname or IP address that should be given to bound applications for connecting to databases managed by the service. 
    * This hostname or IP address should resolve to your load balancer and be considered long-lived. 
    * When this field is modified, applications must be rebound to receive updated credentials.

To enable their PCF apps to use a MySQL database, developers bind their apps to instances of the MySQL for PCF service. For more information, see [Application Binding](http://docs.pivotal.io/pivotalcf/devguide/services/index.html#application-binding). By default, the MySQL service provides bound apps with the IP address of the first instance in the proxy tier, even with multiple proxy instances deployed. This makes the first proxy instance a single point of failure unless you deploy a load balancer.

<p class="note"><strong>Note</strong>: To eliminate the first proxy instance as a single point of failure, configure a load balancer to route client connections to all proxy IPs, and configure the MySQL service to give bound applications a hostname or IP address that resolves to the load balancer.</p>

### <a id="proxy-instances"></a> Proxy Count Cannot be Reduced

Once an operator deploys MySQL for PCF, they cannot reduce the number of proxy IPs or proxy instances, and cannot remove the configured IPs from the **Proxy IPs** field.

If the product is initially deployed without proxy IPs, adding IPs to the **Proxy IPs** field can only add additional proxy instances. Scaling down is unpredictably permitted, and the first proxy instance can never be assigned an operator-configured IP.

## <a id="server"></a>MySQL Server Configuration

![Configure MySQL Server Configuration](config-servers.png)

You can change the following options to suit your environment. Since these changes affect all service instances, exercise caution when making changes.

For more information about how MySQL for PCF is configured, see the [architecture documentation](architecture.html).

- <a id="innodb-strict-mode"></a>**Enable InnoDB Strict Mode**

    Default: Enabled

    See the [architecture documentation](architecture.html#innodb-strict-mode) for information on this setting.

- <a id="local-infile"></a>**Allow Clients to Send Files**

    Default: Enabled

    See the [architecture documentation](architecture.html#local-infile) for information on this setting.

- <a id="command-history"></a>**Allow Command History**

    Default: Enabled

    See the [architecture documentation](architecture.html#command-history) for information on this setting.

- <a id="reverse-dns-lookups"></a>**Disable Reverse DNS lookups**

    Default: Enabled

    See the [architecture documentation](architecture.html#reverse-dns-lookups) for information on this setting.

- <a id="remote-admin"></a>**Allow Remote Admin Access**

    Default: Disabled

    See the [architecture documentation](architecture.html#remote-admin) for information on how to configure this setting.
    Enabling this checkbox affects how you back up and restore your MySQL data manually.
    See [Perform Manual Backup](backup.html#manual-process).

- **Read-Only User Password**

    Default: Disabled

    Activates a special user, `roadmin`, a read-only administrator. Supply a special password for administrators who require the ability to view all of the data maintained by the MySQL for PCF installation. Leaving this field blank de-activates the read-only user.

- **MySQL Start Timeout**

    Default: 60 seconds

    The maximum amount of time necessary for the MySQL process to start, in seconds. When restarting the MySQL server processes, there are conditions under which the process takes longer than expected to appear as running. This can cause parts of the system automation to assume that the process failed to start properly, and appear as failing in Ops Manager and BOSH output. Depending on the data stored by the database, and the time represented in logs, you may need to increase this above the default of 60 seconds.

- <a id="cluster-probe-timeout"></a>**New Cluster Probe Timeout**

    Default: 10 seconds

    There is special logic to detect if a starting node is the first node of a new installation, or if it is part of an existing cluster. Part of this logic is to probe if the other nodes of the cluster have already been deployed. In cases of high latency, this timeout period may be too long. When these probes take too long to respond, it's possible that the **MySQL Start Timeout** might fail the deployment. To account for this, either increase the **MySQL Start Timeout** or lower the **New Cluster Probe Timeout** so that a new node doesn't spend too long performing this test.

- <a id="allow-table-locks"></a>**Allow Table Locks**

    Default: Enabled

    When enabled, clients can acquire table locks on the node processing the transaction.
    This is enabled for backwards compatibility.
    Pivotal recommends disabling this for all new deployments because table locks are not replicated across cluster nodes.

- **Enable Large Indices**

    Default: Enabled

    See the [architecture documentation](architecture.html#innnodb-large-prefix) for information on this setting.

- **Enable replication debug logging**

    Default: Enabled

    Directs MySQL for PCF service to log replication error events. Disable this option only if error logging is overloading your logging systems' capacity.

- **Server Activity Logging**

    The MySQL service includes the [MariaDB Audit plugin](https://mariadb.com/kb/en/mariadb/about-the-mariadb-audit-plugin/) to log server activity. You can disable this plugin, or configure which [events](https://mariadb.com/kb/en/mariadb/about-the-mariadb-audit-plugin/#logging-events) are recorded. The log can be found at `/var/vcap/store/mysql_audit_logs/mysql_server_audit.log` on each VM. When server logging is enabled, the file is rotated every 100 megabytes, and the 30 most recent files are retained.

    - Event types

        Default: `connect,query`

    - Exclude users

        Supply a comma-separated list of additional database users that should not be included in the activity log. Note that these users are always excluded form audit logging:
        - `repcanary`
        - `mysql-metrics`
        - `cluster-health-logger`
        - `galera-healthcheck`
        - `quota-enforcer`

    <p class="note"><strong>Note</strong>: Due to the sensitive nature of these logs, they are not transmitted to the syslog server.</p>

- **Maximum Temporary Table Memory Size**

    Default: 33554432 bytes (32 MB)

    The maximum size (in bytes) of internal in-memory temporary tables. See the [architecture documentation](architecture.html#temp-tables) for information on this setting.

- **Table Open Cache Size**

    Default: 2000 tables

    The number of table handles to keep open. See the [architecture documentation](architecture.html#table-open-cache-size) for information on this setting.

- **Table Definition Cache Size**

    Default: 8192 tables

    Set this to a number relative to the number of tables the server will manage. See the [architecture documentation](architecture.html#table-definition-cache-size) for information on this setting.

- **InnoDB Buffer Pool Size**

    Default: 50 percent of instance RAM

    Choose Percent to configure the percent of system RAM allocated to the memory buffer used by InnoDB. Choose Bytes to configure the size in bytes, instead. See the [architecture documentation](architecture.html#innodb-buffer-pool-size) for information on this setting.

- <a id="innodb-flush-log-at-trx-commit"></a>**InnoDB Log Flush Timing**

    Default: 2 

    InnoDB log buffer is written to the log file at each commit;
    the log file is flushed to disk once per second.
    Set to `1` when running in non-HA mode.
    This default improves cluster performance.
    For more information, see the [InnoDB Flush Method](architecture.html#innodb-flush-method).
    The MariaDB default, `1`, is to flush the log to disk at every commit,
    which is required for ACID compliance.

    <p class="note"><strong>Note</strong>: The most recent second's transactions is flushed to disk <i>only</i> if all nodes crash simultaneously.
       All other cluster lifecycle events do not affect data retention.
       To eliminate this risk, set Log Flush Timing to <code>1</code>,
       which improves the performance of heavily active clusters.</p>

- **Maximum Server Connections**

    Default: 1500

    The maximum number of simultaneous client connections for the entire deployment, regardless of service instance.

- **Binary Log Retention Time**

    Default: 7 days

    Time in days to store binary logs before purging. These logs are not used by MySQL for PCF. They are stored only for diagnostic purposes.

## <a id="backups"></a>Backups

![Configure Backups](config-backups.png)

The **Backups** pane configures automated database backups. To configure backups:

1. Choose **Disable Backups** or **Enable Backups**.  If you enable automated backups, follow the instructions in the [Backups](./backup.html) topic to configure the backups.

2. Ensure that the **Instances** setting for **Backup Prepare Node** in the **Resource Config** pane matches your automated backups enable or disable choice:
  - **Disable Backups** selected: set **Backup Prepare Node** > **Instances** to `0`.
  - **Enable Backups** selected: set **Backup Prepare Node** > **Instances** to `1`.

## <a id="advanced"></a>Advanced Options

![Configure Advanced Options](config-advanced.png)

The **Advanced Options** pane lets you configure the following features:

- The Replication Canary: For more information, see [Monitoring the MySQL Service](monitoring-mysql.html#repcanary).

- The Interruptor: For more information, see [Using the Interruptor](interruptor.html).

- **Quota Enforcer Frequency**

    By default, the Quota Enforcer polls for violators and reformers every 30 seconds. This setting, in seconds, changes how long the quota enforcer pauses between checks.
    Quota Enforcer polling draws minimal resources, but if you want to reduce this load, increase this interval. Be aware, however, that a long Quota Enforcer interval may cause apps to write more data than their pre-determined limit allows.

- <a id="cluster-name"></a>**Cluster Name**

    Default: `cf-mariadb-galera-cluster`

    Set a unique name for the cluster. When examining the state or making manual changes to a Pivotal MySQL cluster, it's useful to check the name of the cluster. You can see the name of the cluster by running the query, `select @@wsrep_cluster_name`.

    <p class="note"><strong>Important:</strong> may only be set during initial deployment. Changing this field after the cluster has been deployed causes <strong>Apply Changes</strong> to fail.</p>

    Please contact Pivotal Support for help in changing the name of a cluster that has already been deployed.

## <a id="errands"></a>Errands

Two post-deploy errands run by default: the **broker registrar** and the **smoke test**.
The broker registrar errand registers the broker with the Cloud Controller and makes the service plan public.
The smoke test errand runs basic tests to validate that service instances can be created and deleted,
and that apps pushed to Pivotal Application Service (PAS) or Elastic Runtime can be bound and write to MySQL service instances.
You can turn both errands on or off in the **Errands** pane under the **Settings** tab.

<p class="note"><strong>Note</strong>: Disabling errands can result in unexpected side effects. <strong>Do not</strong> reconfigure errands in your deployment without instruction from Pivotal Support.

Additionally, the <strong>Errands</strong> pane also shows a <strong>broker-deregistrar</strong> pre-delete errand.
Ops Manager runs the broker-deregistrar errand to clean up when it uninstalls a tile.
</br></br>Depending on your Pivotal Cloud Foundry (PCF) version, use the appropriate command for either BOSH CLI v1 or BOSH CLI v2. Running the following command under any other circumstances deletes user data. <strong>Do not</strong> run this errand unless instructed to do so by Pivotal Support:

</br></br>On PCF v1.10, run the BOSH CLI v1 command:
</br><code>bosh run errand broker-registrar</code>

</br>On PCF v1.11 or later, run the BOSH CLI v2 command:
</br><code>bosh2 run-errand broker-registrar</code>
</p>

## <a id="resource-config"></a>Resource Config

![Configure Resources](config-resources.png)

This pane configures the number of instances, persistent disk capacity, and VM type for all component VMs that run the MySQL for PCF service.

Make sure to provision ample resources for your **MySQL Server** nodes. MariaDB servers require sufficient CPU, RAM, and IOPS to promptly respond to client requests. Also note that the MySQL for PCF reserves about 2-3 GB of each instance's persistent disk for service operations use. The rest of the capacity is available for the databases. The MariaDB cluster nodes are configured by default with 100GB of persistent disk. The deployment fails if this is less than 3GB; we recommend allocating 10GB minimum.

If **Enable Backups** is selected in the **Backups** pane, set **Backup Prepare Node** > **Instances** to `1`.

The following topics describe common scenarios where you may want to edit the **Resource Config** tab:

- [Redeploying with Larger Persistent Disks](#disk) 
- [Switch Between Single and HA Topologies](#switch-topologies)

### <a id="disk"></a> Redeploying with Larger Persistent Disks

#### Monitor Available Disk Space

<p class="note"><strong>Important:</strong> MySQL does not gracefully handle the condition where there is no available free space on either ephemeral or persistent disks.
   Therefore, you must closely monitor free disk space. </p>

There are two circumstances in which you might need to redeploy with larger persistent disks:

1. So much data is stored on the disk that very little free space remains. 
2. The Service Broker allocated so many service plans that there is insufficient reserved space to allocate to new service instances.

#### Redeploy with Larger Persistent Disks

If the [Persistent Disk](./monitoring-mysql.html#persistent-disk) metric is over 75% full (warning) or over 90% full (critical),
you should redeploy the MySQL for PCF tile with larger persistent disks.

To redeploy with larger persistent disks, do the following:

1. Click the **Resource Config** tab in the MySQL for PCF Settings page.
1. Locate the **MySql Server** job in the top row, then increase the size in the second column of that row.
    <br>![Increase Disk Size](config-resources-disk.png)
1. Click **Save**.
1. Return to the Ops Manager dashboard and click **Apply Changes**.

During redeployment, BOSH takes time to copy data from the old disk configuration to the new one.
When running in single-node mode, the service is not available during this time.
When running in clustered mode, if `max_in_flight` is set to 1 (the default),
the service remains available as long as there is sufficient disk space for normal operation.

#### Recovering from Issues with Persistent Disk(s)

Problems with persistent disks are typically caused by:

* Infrastructure failure---for example, an administrator disables, deletes, or detaches a disk. 
* Hardware failure---for example, a disk physically breaks. 

The MariaDB nodes store their data on the persistent disk, so when recovering from a disk issue you must either reattach or recreate the disk before restarting the node.

To determine what type of recovery is required, determine the state of the Galera cluster.
For instructions, see [Check Cluster State](./troubleshooting.html#check-state).

**If the Cluster has Failed**

If the cluster has lost quorum, that is, less than half of the nodes can communicate, the rest of the nodes automatically shut down.
In this case:

1. Repair or replace any corrupted or lost persistent disks:
   * If you are using PCF v1.11 or later, follow the [BOSH CLI v2](#recoverdisk-cli2) procedures to recover a disk. 
   * If you are using PCF v1.10, follow the [BOSH CLI v1](#recoverdisk-cli1) procedures to recover a disk. 

1. Bootstrap the cluster to restart it.
   For more information, see [Bootstrapping](./bootstrapping.html).

**If the Cluster is Still Running**

If clustering has quorum, that is, at least half of the nodes can communicate, an administrator must only recover the failing nodes.
In this case:

1. Repair or replace any corrupted or lost persistent disks:
   * If you are using PCF v1.11 or later, follow the [BOSH CLI v2](#recoverdisk-cli2) procedures to recover a disk. 
   * If you are using PCF v1.10, follow the [BOSH CLI v1](#recoverdisk-cli1) procedures to recover a disk. 

2. Restart recovered nodes.

The recovered node automatically rejoins the cluster and is updated with the current data.

#### <a id="recoverdisk-cli2"></a> Recover a Disk with the BOSH CLI v2

The steps to recover disks depend on the the nature of the disk failure, described in scenarios A and B below.

**A---When Persistent Disk Is Detached and Can Be Reattached**

When the disk is detached, monit considers the process stopped and BOSH considers the job as failing.
BOSH cloud check does not recognize the disk is unattached. 

To recover a disk in this scenario, do the following:

1. List all VMs managed by the BOSH Director in your MySQL deployment. For example:
    <pre class="terminal">
    $ bosh2 -e my-env -d my-dep vms
    </pre>
1. From the above output, identify the name of the broken node. The name follows the format `mysql_AZ/INDEX`, such as `mysql_z1/0`.
1. Attempt to recreate the broken node using BOSH. This alerts BOSH to the missing disk. For example:
    <pre class="terminal">$ bosh2 -e my-env -d my-dep recreate mysql\_z1/0</pre>
    You should see an error that looks like this:
    <pre class="terminal">
    Error 100: Disk (YOUR\_DISK\_ID) is not attached to VM (vm-fc4ab74e-61ed-4d12-aa93-a1bbb389723f)
    </pre>
    If the recreate command fails with the following error, wait until monit times out and all jobs are stopped, and then try again.
    <pre class="terminal">
    Failed updating job mysql\_AZ: mysql\_AZ/INDEX (canary) (00:00:22): Action Failed get_task: Task 8ace0778-c5aa-4a2f-55a0-42443452adb1 result: Stopping Monitored Services: Stopping service gra-log-purger-executable: Stopping Monit service gra-log-purger-executable: Request failed with 503 Service Unavailable:
    </pre>

1. Use BOSH cloud check to reattach the disk. For example:
  <pre class="terminal">
  $ bosh2 -e my-env -d my-dep cck
  </pre>
  When prompted, choose `3. Reattach disk and reboot instance`.
1. Restart the node.
   When the node is restarted, it joins the cluster. For example:
  <pre class="terminal">
  $ bosh2 -e my-env -d my-dep restart mysql_z1/0
  </pre>

<br>**B---When Persistent Disk Is Lost and Must Be Recreated**

If you attempt to use BOSH cloud check to reattach the disk and you see the following error, then you know the disk is lost.
<pre class="terminal">
$ bosh2 -e my-env -d my-dep cck
...
Failed: File []/vmfs/volumes/volume-id/datastore-id/disk-id.vmdk was not found
</pre>

To recover a disk in this scenario, do the following:

1. SSH into `cf-mysql-broker` instances and stop the processes.
   This prevents creation and deletion of instances attempting to recreate the broken node to get its disk ID.
  <pre class="terminal">
  $ sudo monit stop all
  </pre>
1. SSH into proxy instances and stop the processes.
   This prevents data from changing on the broken node when attempting to recreate it to get its disk ID.
  <pre class="terminal">
  $ sudo monit stop all
  </pre>
1. Attempt to recreate the node using BOSH in order to obtain the disk ID. For example:
  <pre class="terminal">
  $ bosh2 -e my-env -d my-dep recreate mysql_z1/0
  </pre>

    You should see an error that looks like this:
  <pre class="terminal">
  Error 100: Disk (YOUR\_DISK\_ID) is not attached to VM (vm-fc4ab74e-61ed-4d12-aa93-a1bbb389723f)
  </pre>

    Make a note of the value of `YOUR_DISK_ID` in order to use it in the next step.

1. Connect to the BOSH Director Postgres database and remove references to the disk.
  On a microbosh the database is local.
  <pre class="terminal">
  $ /var/vcap/packages/postgres/bin/psql -U postgres --password bosh2
  psql (9.0.3)
  Type "help" for help.
  bosh2=> DELETE FROM persistent\_disks WHERE disk\_cid = 'YOUR\_DISK\_ID';
  DELETE 1
  bosh2=> DELETE FROM vsphere\_disk WHERE uuid = 'YOUR\_DISK\_ID';
  DELETE 1
  </pre>

    This deletes the reference to the lost disk
    and causes BOSH to recreate a fresh disk for this VM on the next deploy.
1. Through your infrastructure interface (for example, vCenter client or AWS console),
   power off and delete the VM corresponding to `mysql_AZ/INDEX`.
1. Use BOSH cloud check to remove reference to the VM. For example:
  <pre class="terminal">
  $ bosh2 -e my-env -d my-dep cck
  </pre>
  When prompted, choose `3. Delete VM reference (DANGEROUS!)`.
1. Edit the deployment manifest and reduce the number of instances of both `cf-mysql-broker` and `proxy` to 0, and remove the static IP for `proxy`. This prevents new connections from being made to the node after it is recreated but before it joins the cluster.
1. Deploy the release to recreate the node and remove the broker and proxy, specifying the name of the manifest file. For example:
  <pre class="terminal">
  $ bosh2 -e my-env -d my-dep deploy manifest.yml
  </pre>
1. SSH into any of the nodes and verify that all nodes have joined the cluster. For instructions, see [Check Cluster State](./troubleshooting.html#check-state).
1. After all nodes have joined the cluster, edit the deployment manifest to set the number of instances for `cf-mysql-broker` and `proxy` back to 1 and restore the static IP for `proxy`. 
1. Redeploy the release. For example:
  <pre class="terminal">
  $ bosh2 -e my-env -d my-dep deploy manifest.yml
  </pre>

#### <a id="recoverdisk-cli1"></a> Recover a Disk with the BOSH CLI v1

The steps to recover disks depend on the the nature of the disk failure, described in scenarios A and B below.

**A---When Persistent Disk Is Detached and Can Be Reattached**

When the disk is detached, monit considers the process stopped and BOSH considers the job as failing.
BOSH cloud check does not recognize the disk is unattached. 

To recover a disk in this scenario, do the following:

1. Attempt to recreate the broken node using BOSH.
   This alerts BOSH to the missing disk.
  <pre class="terminal">
  $ bosh recreate mysql_AZ INDEX
  </pre>

      You should see an error that looks like this:
  <pre class="terminal">
  Error 100: Disk (YOUR\_DISK\_ID) is not attached to VM (vm-fc4ab74e-61ed-4d12-aa93-a1bbb389723f)
  </pre>
  If recreate fails with the following error, wait until monit times out and all jobs are stopped then try again.
  <pre class="terminal">
  Failed updating job mysql\_AZ: mysql\_AZ/INDEX (canary) (00:00:22): Action Failed get_task: Task 8ace0778-c5aa-4a2f-55a0-42443452adb1 result: Stopping Monitored Services: Stopping service gra-log-purger-executable: Stopping Monit service gra-log-purger-executable: Request failed with 503 Service Unavailable:
  </pre>
1. Use BOSH cloud check to reattach the disk.
  <pre class="terminal">
  $ bosh cck
  </pre>
  When prompted, choose `3. Reattach disk and reboot instance`.
1. Restart the node.
   When the node is restarted, it joins the cluster.
  <pre class="terminal">
  $ bosh restart mysql_AZ INDEX
  </pre>

<br>**B---When Persistent Disk Is Lost and Must Be Recreated**

If you attempt to use BOSH cloud check to reattach the disk and you see the following error, then you know the disk is lost.
<pre class="terminal">
$ bosh cck
...
Failed: File []/vmfs/volumes/volume-id/datastore-id/disk-id.vmdk was not found
</pre>

To recover a disk in this scenario, do the following:

1. SSH into `cf-mysql-broker` instances and stop the processes.
   This prevents creation and deletion of instances attempting to recreate the broken node to get its disk ID.
  <pre class="terminal">
  $ sudo monit stop all
  </pre>
1. SSH into proxy instances and stop the processes.
   This prevents data from changing on the broken node when attempting to recreate it to get its disk ID.
  <pre class="terminal">
  $ sudo monit stop all
  </pre>
1. Attempt to recreate the node using BOSH in order to obtain the disk ID.
  <pre class="terminal">
  $ bosh recreate mysql_AZ INDEX
  </pre>

    You should see an error that looks like this:
  <pre class="terminal">
  Error 100: Disk (YOUR\_DISK\_ID) is not attached to VM (vm-fc4ab74e-61ed-4d12-aa93-a1bbb389723f)
  </pre>

    Make a note of the value of `YOUR_DISK_ID` in order to use it in the next step.

1. Connect to the BOSH Director Postgres database and remove references to the disk.
  On a microbosh the database is local.
  <pre class="terminal">
  $ /var/vcap/packages/postgres/bin/psql -U postgres --password bosh
  psql (9.0.3)
  Type "help" for help.
  bosh=> DELETE FROM persistent\_disks WHERE disk\_cid = 'YOUR\_DISK\_ID';
  DELETE 1
  bosh=> DELETE FROM vsphere\_disk WHERE uuid = 'YOUR\_DISK\_ID';
  DELETE 1
  </pre>

    This deletes the reference to the lost disk
    and causes BOSH to recreate a fresh disk for this VM on the next deploy.
1. Through your infrastructure interface (for example, vCenter client or AWS console),
   power off and delete the VM corresponding to `mysql_AZ/INDEX`.
1. Use BOSH cloud check to remove reference to the VM.
  <pre class="terminal">
  $ bosh cck
  </pre>
  When prompted, choose `3. Delete VM reference (DANGEROUS!)`.
1. Edit the deployment manifest and reduce the number of instances of both `cf-mysql-broker` and `proxy` to 0, and remove the static IP for `proxy`. This prevents new connections from being made to the node after it is recreated but before it joins the cluster.
1. Deploy the release to recreate the node and remove the broker and proxy.
  <pre class="terminal">
  $ bosh deploy
  </pre>
1. SSH into any of the nodes and verify that all nodes have joined the cluster. For instructions, see [Check Cluster State](./troubleshooting.html#check-state).
1. After all nodes have joined the cluster, edit the deployment manifest to set the number of instances for `cf-mysql-broker` and `proxy` back to 1 and restore the static IP for `proxy`. 
1. Redeploy the release.
  <pre class="terminal">
  $ bosh deploy
  </pre>

### <a id="switch-topologies"></a> Switch Between Single and HA Topologies

To switch your MySQL for PCF service between single-node and high availability (HA) topologies, navigate to the **Resource Config** pane and change the **Instances** settings for the **MySQL Server**, **Proxy**, and **Service Broker** components as shown in the table below.

For information on proxy settings to use with load balancers, see [Proxy](#proxy).

<table>
<tr><th>Resource</th><th>Single-Node</th><th>High Availability (HA)</th></tr>
<tr><td>MySQL Server</td><td>1</td><td>3</td></tr>
<tr><td>Proxy</td><td>1</td><td>2</td></tr>
<tr><td>Service Broker</td><td>1</td><td>1-2<sup>*</sup></td></tr>
</table>

<sup>*</sup>Routine database operations do not require two service brokers.


Single and three-node clusters are the only supported topologies. Ops Manager  allows you to set the number of **MySQL Server** instances to other values, but Pivotal recommends only one or three.

If you scale up to three MySQL nodes, Pivotal recommends spreading them across different Availability Zones to maximize cluster availability. An Availability Zone (AZ) is a network-distinct section of a region. For more information about Amazon AZs, see [Amazon's documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html).

When you change the instance counts for a MySQL service, a top-level property is updated with the new nodes' IP addresses. As BOSH deploys, it updates the configuration and restarts all of the MySQL nodes **and** the proxy nodes (to inform them of the new IP addresses as well). Restarting the nodes causes all connections to that node to be dropped while the node restarts.

## <a id="stemcell"></a>Stemcell

This pane uploads the stemcell that you want the service components to run on. Find available stemcells at [Pivotal Network](http://network.pivotal.io).
